{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He added that the scheme aims to attract regional and Malaysian families to manage their family wealth from Malaysia. Supported by good infrastructure, a competitive talent pool, robust common law practices and effective governance, opportunities abound for family offices,” said Mr Amir Hamzah. This scheme is aimed at being operational by the first quarter of 2025, he added. During a press conference after his speech, Mr Amir Hamzah explained that these companies will be subject to zero per cent tax when they file revenue of their transactions for 10 years.He added that the incentives can then be extended if the family offices scale up investments and assets of their operations in Malaysia. Mr Amir Hamzah added that a key criteria is that these family offices must have minimum assets of RM70 million (US$16.73 million), and a portion of these must be invested in Malaysia’s economy. \n"
     ]
    }
   ],
   "source": [
    "# create a text with 1024 characters\n",
    "\n",
    "text= [\"He added that the scheme aims to attract regional and Malaysian families to manage their family wealth from Malaysia. Supported by good infrastructure, a competitive talent pool, robust common law practices and effective governance, opportunities abound for family offices,” said Mr Amir Hamzah. This scheme is aimed at being operational by the first quarter of 2025, he added. During a press conference after his speech, Mr Amir Hamzah explained that these companies will be subject to zero per cent tax when they file revenue of their transactions for 10 years.He added that the incentives can then be extended if the family offices scale up investments and assets of their operations in Malaysia. Mr Amir Hamzah added that a key criteria is that these family offices must have minimum assets of RM70 million (US$16.73 million), and a portion of these must be invested in Malaysia’s economy. \"]\n",
    "for i in text:\n",
    "    print(i)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_core-0.3.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "     ---------------------------------------- 0.0/149.4 kB ? eta -:--:--\n",
      "     -------- ------------------------------ 30.7/149.4 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------- --------- 112.6/149.4 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 149.4/149.4 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.7-cp311-none-win_amd64.whl.metadata (51 kB)\n",
      "     ---------------------------------------- 0.0/51.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.7/51.7 kB ? eta 0:00:00\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.23.4-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (2.1)\n",
      "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.0 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.4/1.0 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.5/1.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.7/1.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 0.9/1.0 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.2-py3-none-any.whl (399 kB)\n",
      "   ---------------------------------------- 0.0/399.7 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 225.3/399.7 kB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 399.7/399.7 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.125-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 0.0/290.2 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 204.8/290.2 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 290.2/290.2 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.9 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 266.2/434.9 kB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 434.9/434.9 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.23.4-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/1.9 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/1.9 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/1.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 5.3 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.7-cp311-none-win_amd64.whl (137 kB)\n",
      "   ---------------------------------------- 0.0/137.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 137.3/137.3 kB 7.9 MB/s eta 0:00:00\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: pydantic-core, packaging, orjson, jsonpatch, h11, annotated-types, pydantic, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.12\n",
      "    Uninstalling pydantic-1.10.12:\n",
      "      Successfully uninstalled pydantic-1.10.12\n",
      "Successfully installed annotated-types-0.7.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 langchain-0.3.0 langchain-core-0.3.2 langchain-text-splitters-0.3.0 langsmith-0.1.125 orjson-3.10.7 packaging-24.1 pydantic-2.9.2 pydantic-core-2.23.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He added that the scheme aims to attract regional and Malaysian families to manage their family wealth from Malaysia. Supported by good infrastructure, a competitive talent pool, robust common law practices and effective governance, opportunities abound for family offices,” said Mr Amir Hamzah. This scheme is aimed at being operational by the first quarter of 2025, he added. During a press conference after his speech, Mr Amir Hamzah explained that these companies will be subject to zero per cent tax when they file revenue of their transactions for 10 years.He added that the incentives can then be extended if the family offices scale up investments and assets of their operations in Malaysia. Mr Amir Hamzah added that a key criteria is that these family offices must have minimum assets of RM70 million (US$16.73 million), and a portion of these must be invested in Malaysia’s economy.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['He added that the scheme',\n",
       " 'aims to attract regional',\n",
       " 'and Malaysian families',\n",
       " 'to manage their family',\n",
       " 'wealth from Malaysia.',\n",
       " 'Supported by good',\n",
       " 'good infrastructure, a',\n",
       " 'a competitive talent',\n",
       " 'pool, robust common law',\n",
       " 'law practices and',\n",
       " 'and effective',\n",
       " 'governance,',\n",
       " 'opportunities abound for',\n",
       " 'for family offices,”',\n",
       " 'said Mr Amir Hamzah.',\n",
       " 'This scheme is aimed at',\n",
       " 'at being operational by',\n",
       " 'by the first quarter of',\n",
       " 'of 2025, he added.',\n",
       " 'During a press',\n",
       " 'conference after his',\n",
       " 'his speech, Mr Amir',\n",
       " 'Amir Hamzah explained',\n",
       " 'that these companies',\n",
       " 'will be subject to zero',\n",
       " 'zero per cent tax when',\n",
       " 'when they file revenue',\n",
       " 'of their transactions',\n",
       " 'for 10 years.He added',\n",
       " 'that the incentives can',\n",
       " 'can then be extended if',\n",
       " 'if the family offices',\n",
       " 'scale up investments and',\n",
       " 'and assets of their',\n",
       " 'operations in Malaysia.',\n",
       " 'Mr Amir Hamzah added',\n",
       " 'that a key criteria is',\n",
       " 'is that these family',\n",
       " 'offices must have',\n",
       " 'have minimum assets of',\n",
       " 'of RM70 million',\n",
       " '(US$16.73 million), and',\n",
       " 'and a portion of these',\n",
       " 'must be invested in',\n",
       " 'in Malaysia’s economy.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example for character-level splitting\n",
    "text_splitter = CharacterTextSplitter(chunk_size=25, chunk_overlap=5)\n",
    "chunks = text_splitter.split_text(text[0])\n",
    "print(chunks)\n",
    "\n",
    "# Example for word-based splitting (Recursive splitting)\n",
    "word_splitter = RecursiveCharacterTextSplitter(chunk_size=25, chunk_overlap=5)\n",
    "word_chunks = word_splitter.split_text(text[0])\n",
    "print(word_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He added that the scheme aims to attract regional and Malaysian families to manage their family wealth from Malaysia. Supported by good infrastructure, a competitive talent pool, robust common law practices and effective governance, opportunities abound for family offices,” said Mr Amir Hamzah. This scheme is aimed at being operational by the first quarter of 2025, he added. During a press conference after his speech, Mr Amir Hamzah explained that these companies will be subject to zero per cent tax when they file revenue of their transactions for 10 years.He added that the incentives can then be extended if the family offices scale up investments and assets of their operations in Malaysia. Mr Amir Hamzah added that a key criteria is that these family offices must have minimum assets of RM70 million (US$16.73 million), and a portion of these must be invested in Malaysia’s economy.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:/DataScience_For_mySelf/Projects_myself/RagMLOPS/TriModalRAG_System\n",
      "d:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS\\TriModalRAG_System\\data\\image\n",
      "d:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS\\TriModalRAG_System\\data\\image\\weather_images_0.png\n",
      "weather_images_0.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "ROOT_PROJECT_DIR = os.getcwd().split(\"\\\\\")[:-1]\n",
    "ROOT_PROJECT_DIR = \"/\".join(ROOT_PROJECT_DIR)\n",
    "print(ROOT_PROJECT_DIR)\n",
    "text_dir = Path(ROOT_PROJECT_DIR + \"/data/image\")\n",
    "print(text_dir)\n",
    "# print(text_dir.glob(\"*.png\"))\n",
    "for text_p in text_dir.glob(\"*.png\"):\n",
    "    print(text_p)\n",
    "    text_url = os.path.basename(text_p)\n",
    "    print(text_url)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS\\TriModalRAG_System\\data\\image\n",
      "d:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS\\TriModalRAG_System\\data\\image\\src\\artifacts\\data\\embeddings\n"
     ]
    }
   ],
   "source": [
    "text_dir = Path(ROOT_PROJECT_DIR + \"/data/image\")\n",
    "print(text_dir)\n",
    "text_dir = text_dir /  (\"src/\"  \"artifacts/\" + \"data/\" + \"embeddings/\")\n",
    "print(text_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text_url': 'D:\\\\DataScience_For_mySelf\\\\Projects_myself\\\\RagMLOPS…',\n",
       "  'type': 'weather',\n",
       "  'title': '0_1406_3726v1.pdf'},\n",
       " {'text_url': 'D:\\\\DataScience_For_mySelf\\\\Projects_myself\\\\RagMLOPS…',\n",
       "  'type': 'weather',\n",
       "  'title': '1_1911_09001v1.pdf'},\n",
       " {'text_url': 'D:\\\\DataScience_For_mySelf\\\\Projects_myself\\\\RagMLOPS…',\n",
       "  'type': 'weather',\n",
       "  'title': '0_1406_3726v1.pdf'},\n",
       " {'text_url': 'D:\\\\DataScience_For_mySelf\\\\Projects_myself\\\\RagMLOPS…',\n",
       "  'type': 'weather',\n",
       "  'title': '1_1911_09001v1.pdf'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text_urls = [\n",
    "    \"D:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS…\",\n",
    "    \"D:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS…\",\n",
    "    \"D:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS…\",\n",
    "    \"D:\\DataScience_For_mySelf\\Projects_myself\\RagMLOPS…\",\n",
    "]\n",
    "_types = [\n",
    "    \"weather\",\n",
    "    \"weather\",\n",
    "    \"weather\",\n",
    "    \"weather\",\n",
    "]\n",
    "titles = [\n",
    "    \"0_1406_3726v1.pdf\",\n",
    "    \"1_1911_09001v1.pdf\",\n",
    "    \"0_1406_3726v1.pdf\",\n",
    "    \"1_1911_09001v1.pdf\"\n",
    "]\n",
    "import pandas as pd\n",
    "\n",
    "pyloads = pd.DataFrame({\"text_url\": _text_urls,\n",
    "                                        \"type\": _types,\n",
    "                                        \"title\": titles})\n",
    "\n",
    "\n",
    "pyload_dicts = pyloads.to_dict(orient=\"records\")\n",
    "pyload_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_url': 'D:\\\\DataScience_For_mySelf\\\\Projects_myself\\\\RagMLOPS…',\n",
       " 'type': 'weather',\n",
       " 'title': '0_1406_3726v1.pdf'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyload_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pyload_dicts\u001b[38;5;241m.\u001b[39mitems())):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "for x in range(len(pyload_dicts.items())):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\downloads\\program files\\anaconda\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/436.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 112.6/436.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 256.0/436.4 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 436.4/436.4 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.0/286.0 kB 17.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 32.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 15.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.1 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2023, 2003, 1037, 3231, 6251, 1012,  102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'a', 'test', 'sentence', '.', '[SEP]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "# Example input text\n",
    "text = \"This is a test sentence.\"\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the token IDs\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "print(input_ids)\n",
    "\n",
    "# Get embeddings from the BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the last hidden state (embeddings)\n",
    "embeddings = outputs.last_hidden_state  # shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "# Convert input IDs back to tokens (for verification)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "# tokens_tesst = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "# print(tokens_tesst)\n",
    "\n",
    "# Print tokens and their corresponding embeddings\n",
    "tokens_list = [token for token, embedding in zip(tokens, embeddings[0])]\n",
    "tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = [[[1, 2, 3]],\n",
    "     [[4, 5, 6]],\n",
    "     [[1, 2, 3]],\n",
    "     [[4, 5, 6]],\n",
    "     ] # 2 x 1 x 3\n",
    "\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = 5\n",
    "b = \"ung\"\n",
    "c = True\n",
    "x = (a, b, c)\n",
    "print(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3]],\n",
       "\n",
       "        [[4, 5, 6]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[4, 5, 6]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor(x)\n",
    "x1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m y \u001b[38;5;241m=\u001b[39m [[[ \u001b[38;5;241m4.2606476e-03\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.2611354e-02\u001b[39m,  \u001b[38;5;241m1.5845066e-01\u001b[39m,\n\u001b[0;32m      2\u001b[0m          \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7.3490776e-02\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6.2923972e-03\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.2687368e-01\u001b[39m],\n\u001b[0;32m      3\u001b[0m         [ \u001b[38;5;241m7.1950510e-02\u001b[39m,  \u001b[38;5;241m1.1587572e-04\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9.6582770e-02\u001b[39m,\n\u001b[0;32m      4\u001b[0m          \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.8304415e-01\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5.2026648e-02\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.3342978e-02\u001b[39m]]], \n\u001b[0;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.09848069\u001b[39m,  \u001b[38;5;241m0.06043535\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.04723791\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1566387\u001b[39m ,\n\u001b[0;32m      6\u001b[0m           \u001b[38;5;241m0.01780085\u001b[39m,  \u001b[38;5;241m0.00295972\u001b[39m],\n\u001b[0;32m      7\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.094222\u001b[39m  ,  \u001b[38;5;241m0.05571611\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.04469165\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.13713397\u001b[39m,\n\u001b[0;32m      8\u001b[0m           \u001b[38;5;241m0.02163608\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.00989413\u001b[39m]]]\n\u001b[1;32m---> 10\u001b[0m z \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mconcatenate(x, y)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m z:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "[array([[[ 4.2606476e-03, -2.2611354e-02,  1.5845066e-01,\n",
    "         -7.3490776e-02, -6.2923972e-03, -2.2687368e-01],\n",
    "        [ 7.1950510e-02,  1.1587572e-04, -9.6582770e-02,\n",
    "         -1.8304415e-01, -5.2026648e-02, -4.3342978e-02]]]),array( [[[-0.09848069,  0.06043535, -0.04723791,  -0.1566387 ,\n",
    "          0.01780085,  0.00295972],\n",
    "        [-0.094222  ,  0.05571611, -0.04469165,  -0.13713397,\n",
    "          0.02163608, -0.00989413]]])]\n",
    "\n",
    "z = numpy.concatenate(x, y)\n",
    "\n",
    "for i in z:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world how are you today My name is John Doe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = [\"hello\", \"world\", \"how\", \"are\", \"you\", \"today\", \"My\", \"name\", \"is\", \"John\", \"Doe\"]\n",
    "\n",
    "def test_function(docs):\n",
    "    return \" \".join(doc for doc in docs)\n",
    "\n",
    "test_function(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
